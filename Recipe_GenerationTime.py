# -*- coding: utf-8 -*-
"""Recipe_Gen_Time.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MqQVQN56cCOFTcII_JhtXpSYXqM_GQIO

<div align="center">

  <a href="https://ultralytics.com/yolov5" target="_blank">
    <img width="1024", src="https://raw.githubusercontent.com/ultralytics/assets/main/yolov5/v70/splash.png"></a>


<br>
  <a href="https://bit.ly/yolov5-paperspace-notebook"><img src="https://assets.paperspace.io/img/gradient-badge.svg" alt="Run on Gradient"></a>
  <a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/classify/tutorial.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
  <a href="https://www.kaggle.com/ultralytics/yolov5"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" alt="Open In Kaggle"></a>
<br>

This <a href="https://github.com/ultralytics/yolov5">YOLOv5</a> ðŸš€ notebook by <a href="https://ultralytics.com">Ultralytics</a> presents simple train, validate and predict examples to help start your AI adventure.<br>See <a href="https://github.com/ultralytics/yolov5/issues/new/choose">GitHub</a> for community support or <a href="https://ultralytics.com/contact">contact us</a> for professional support.

</div>

# Setup

Clone GitHub [repository](https://github.com/ultralytics/yolov5), install [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) and check PyTorch and GPU.
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/ultralytics/yolov5  # clone
# %cd yolov5
# %pip install -qr requirements.txt  # install

import torch

import utils

display = utils.notebook_init()  # checks

!unzip /content/train_data.zip -d /content

!python train.py --img 640 --batch 16 --epochs 30 --data /content/data.yaml --weights yolov5s.pt --cache

"""# 1. Predict

`classify/predict.py` runs YOLOv5 Classification inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/predict-cls`. Example inference sources are:

```shell
python classify/predict.py --source 0  # webcam
                              img.jpg  # image
                              vid.mp4  # video
                              screen  # screenshot
                              path/  # directory
                              'path/*.jpg'  # glob
                              'https://youtu.be/LNwODJXcvt4'  # YouTube
                              'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream
```
"""

!python classify/predict.py --weights yolov5s-cls.pt --img 224 --source data/images
# display.Image(filename='runs/predict-cls/exp/zidane.jpg', width=600)

"""&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<img align="left" src="https://user-images.githubusercontent.com/26833433/202808393-50deb439-ae1b-4246-a685-7560c9b37211.jpg" width="600">

# 2. Validate
Validate a model's accuracy on the [Imagenet](https://image-net.org/) dataset's `val` or `test` splits. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag.
"""

# Download Imagenet val (6.3G, 50000 images)
!bash data/scripts/get_imagenet.sh --val

# Validate YOLOv5s on Imagenet val
!python classify/val.py --weights yolov5s-cls.pt --data ../datasets/imagenet --img 224 --half

"""# 3. Train

<p align=""><a href="https://roboflow.com/?ref=ultralytics"><img width="1000" src="https://github.com/ultralytics/assets/raw/main/im/integrations-loop.png"/></a></p>
Close the active learning loop by sampling images from your inference conditions with the `roboflow` pip package
<br><br>

Train a YOLOv5s Classification model on the [Imagenette](https://image-net.org/) dataset with `--data imagenet`, starting from pretrained `--pretrained yolov5s-cls.pt`.

- **Pretrained [Models](https://github.com/ultralytics/yolov5/tree/master/models)** are downloaded
automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases)
- **Training Results** are saved to `runs/train-cls/` with incrementing run directories, i.e. `runs/train-cls/exp2`, `runs/train-cls/exp3` etc.
<br><br>

A **Mosaic Dataloader** is used for training which combines 4 images into 1 mosaic.

## Train on Custom Data with Roboflow ðŸŒŸ NEW

[Roboflow](https://roboflow.com/?ref=ultralytics) enables you to easily **organize, label, and prepare** a high quality dataset with your own custom data. Roboflow also makes it easy to establish an active learning pipeline, collaborate with your team on dataset improvement, and integrate directly into your model building workflow with the `roboflow` pip package.

- Custom Training Example: [https://blog.roboflow.com/train-yolov5-classification-custom-data/](https://blog.roboflow.com/train-yolov5-classification-custom-data/?ref=ultralytics)
- Custom Training Notebook: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1KZiKUAjtARHAfZCXbJRv14-pOnIsBLPV?usp=sharing)
<br>

<p align=""><a href="https://roboflow.com/?ref=ultralytics"><img width="480" src="https://user-images.githubusercontent.com/26833433/202802162-92e60571-ab58-4409-948d-b31fddcd3c6f.png"/></a></p>Label images lightning fast (including with model-assisted labeling)
"""

# Commented out IPython magic to ensure Python compatibility.
# @title Select YOLOv5 ðŸš€ logger {run: 'auto'}
logger = "Comet"  # @param ['Comet', 'ClearML', 'TensorBoard']

if logger == "Comet":
#     %pip install -q comet_ml
    import comet_ml

    comet_ml.init()
elif logger == "ClearML":
#     %pip install -q clearml
    import clearml

    clearml.browser_login()
elif logger == "TensorBoard":
#     %load_ext tensorboard
#     %tensorboard --logdir runs/train

# Train YOLOv5s Classification on Imagenette160 for 3 epochs
!python classify/train.py --model yolov5s-cls.pt --data imagenette160 --epochs 5 --img 224 --cache

"""# 4. Visualize

## Comet Logging and Visualization ðŸŒŸ NEW

[Comet](https://www.comet.com/site/lp/yolov5-with-comet/?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=yolov5_colab) is now fully integrated with YOLOv5. Track and visualize model metrics in real time, save your hyperparameters, datasets, and model checkpoints, and visualize your model predictions with [Comet Custom Panels](https://www.comet.com/docs/v2/guides/comet-dashboard/code-panels/about-panels/?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=yolov5_colab)! Comet makes sure you never lose track of your work and makes it easy to share results and collaborate across teams of all sizes!

Getting started is easy:
```shell
pip install comet_ml  # 1. install
export COMET_API_KEY=<Your API Key>  # 2. paste API key
python train.py --img 640 --epochs 3 --data coco128.yaml --weights yolov5s.pt  # 3. train
```
To learn more about all of the supported Comet features for this integration, check out the [Comet Tutorial](https://docs.ultralytics.com/yolov5/tutorials/comet_logging_integration). If you'd like to learn more about Comet, head over to our [documentation](https://www.comet.com/docs/v2/?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=yolov5_colab). Get started by trying out the Comet Colab Notebook:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RG0WOQyxlDlo5Km8GogJpIEJlg_5lyYO?usp=sharing)

<a href="https://bit.ly/yolov5-readme-comet2">
<img alt="Comet Dashboard" src="https://user-images.githubusercontent.com/26833433/202851203-164e94e1-2238-46dd-91f8-de020e9d6b41.png" width="1280"/></a>

## ClearML Logging and Automation ðŸŒŸ NEW

[ClearML](https://cutt.ly/yolov5-notebook-clearml) is completely integrated into YOLOv5 to track your experimentation, manage dataset versions and even remotely execute training runs. To enable ClearML (check cells above):

- `pip install clearml`
- run `clearml-init` to connect to a ClearML server (**deploy your own [open-source server](https://github.com/allegroai/clearml-server)**, or use our [free hosted server](https://cutt.ly/yolov5-notebook-clearml))

You'll get all the great expected features from an experiment manager: live updates, model upload, experiment comparison etc. but ClearML also tracks uncommitted changes and installed packages for example. Thanks to that ClearML Tasks (which is what we call experiments) are also reproducible on different machines! With only 1 extra line, we can schedule a YOLOv5 training task on a queue to be executed by any number of ClearML Agents (workers).

You can use ClearML Data to version your dataset and then pass it to YOLOv5 simply using its unique ID. This will help you keep track of your data without adding extra hassle. Explore the [ClearML Tutorial](https://docs.ultralytics.com/yolov5/tutorials/clearml_logging_integration) for details!

<a href="https://cutt.ly/yolov5-notebook-clearml">
<img alt="ClearML Experiment Management UI" src="https://github.com/thepycoder/clearml_screenshots/raw/main/scalars.jpg" width="1280"/></a>

## Local Logging

Training results are automatically logged with [Tensorboard](https://www.tensorflow.org/tensorboard) and [CSV](https://github.com/ultralytics/yolov5/pull/4148) loggers to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc.

This directory contains train and val statistics, mosaics, labels, predictions and augmentated mosaics, as well as metrics and charts including precision-recall (PR) curves and confusion matrices.

<img alt="Local logging results" src="https://user-images.githubusercontent.com/26833433/183222430-e1abd1b7-782c-4cde-b04d-ad52926bf818.jpg" width="1280"/>

# Environments

YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):

- **Notebooks** with free GPU: <a href="https://bit.ly/yolov5-paperspace-notebook"><img src="https://assets.paperspace.io/img/gradient-badge.svg" alt="Run on Gradient"></a> <a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a> <a href="https://www.kaggle.com/ultralytics/yolov5"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" alt="Open In Kaggle"></a>
- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)
- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)
- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href="https://hub.docker.com/r/ultralytics/yolov5"><img src="https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker" alt="Docker Pulls"></a>

# Status

![YOLOv5 CI](https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg)

If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit.

# Appendix

Additional content below.
"""

# YOLOv5 PyTorch HUB Inference (DetectionModels only)

model = torch.hub.load(
    "ultralytics/yolov5", "yolov5s", force_reload=True, trust_repo=True
)  # or yolov5n - yolov5x6 or custom
im = "https://ultralytics.com/images/zidane.jpg"  # file, Path, PIL.Image, OpenCV, nparray, list
results = model(im)  # inference
results.print()  # or .show(), .save(), .crop(), .pandas(), etc.

from google.colab import drive
drive.mount('/content/drive')

!unzip -q /content/train_data.zip -d /content/sample

!python train.py --img 640 --batch 16 --epochs 60 --data /content/data.yaml --weights yolov5s.pt --cache

import requests

from models.yolo import Model
from utils.general import check_yaml

# Commented out IPython magic to ensure Python compatibility.
# %cp /content/yolov5/runs/train/exp/weights/best.pt /content/yolov5/runs/train/exp/weights/best_copy.pt

model_cfg = check_yaml('/content/yolov5/models/yolov5s.yaml')  # Ensure this matches the trained model
model = Model(model_cfg)

import torch

# Load the model directly using the YOLOv5 repository
model = torch.hub.load('ultralytics/yolov5', 'custom', path='/content/yolov5/runs/train/exp/weights/best.pt')

# Set model to evaluation mode
model.eval()

# Now you can use the modelÂ forÂ inference

!pip install torch torchvision flask pillow

import cv2
import torch
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

model = torch.hub.load('ultralytics/yolov5', 'custom', path='/content/yolov5/runs/train/exp3/weights/best.pt')
model.eval()

# Load and preprocess the image
image_path = '/content/sample2.jpg'
image = Image.open(image_path)

# Perform object detection
results = model(image)

# Convert results to Pandas DataFrame
results_df = results.pandas().xyxy[0]

# Display the detected objects
print("Detected objects:")
print(results_df)

# Extract bounding boxes and labels
for index, row in results_df.iterrows():
    label = row['name']
    confidence = row['confidence']
    x1, y1, x2, y2 = row[['xmin', 'ymin', 'xmax', 'ymax']]
    print(f"Label: {label}, Confidence: {confidence}, Bounding Box: ({x1}, {y1}), ({x2}, {y2})")

# Display the image with bounding boxes
img_cv = np.array(image)
for index, row in results_df.iterrows():
    x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
    cv2.rectangle(img_cv, (x1, y1), (x2, y2), (0, 255, 0), 2)
    cv2.putText(img_cv, f"{row['name']} {row['confidence']:.2f}", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

# Display the image with Matplotlib
plt.imshow(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.show()

!pip install -q -U google-generativeai

import os
import google.generativeai as genai

def time_input():
  time=int(input())
  return time

os.environ['API_KEY'] = #Enter valid api key.
genai.configure(api_key=os.environ["API_KEY"])
# Initialize the LLM Gemini Model
model = genai.GenerativeModel("gemini-1.5-flash")

ingredients = results_df

# Define your custom prompt
prompt = (
    f"You are an expert chef. I will give you a list of ingredients separated by a semi-colon, "
    f"and you will give me a recipe using all or most of them but not supplementing outside of what might appear on this list. "
    f"If an ingredient is shown as None, ignore it. Give each recipe a catchy title, an approximate time to complete, and a count of people served. "
    f"Also like any recipe, include the ingredient portions in the list labelled Ingredients: and then the Instructions section as a numbered list. "
    f"Here's the list: {ingredients}"
)

time_based_prompt = (
    f"You are an expert chef. I will give you a list of ingredients separated by a semi-colon, "
    f"and you will give me a recipe using all or most of them but not supplementing outside of what might appear on this list. "
    f"Additionally, I will provide you a time limit and you must make sure the recipe that you provide can be completed in that time."
    f"If an ingredient is shown as None, ignore it. Give each recipe a catchy title, an approximate time to complete, and a count of people served. "
    f"Also like any recipe, include the ingredient portions in the list labelled Ingredients: and then the Instructions section as a numbered list. "
    f"Here's the list: {ingredients} and this is the time limit: {time_input()}"
)

cuisine_based_prompt=(
    f"You are an expert chef. I will give you a list of ingredients separated by a semi-colon, "
    f"and you will give me a recipe using all or most of them but not supplementing outside of what might appear on this list. "
    f"Additionally, I want you to provide me with a list of 3 recipes all from unique and different cuisines, but again the recipe must serve the given constraints."
    f"If an ingredient is shown as None, ignore it. Give each recipe a catchy title, an approximate time to complete, and a count of people served. "
    f"Also like any recipe, include the ingredient portions in the list labelled Ingredients: and then the Instructions section as a numbered list. "
    f"Here's the list: {ingredients}"
)
# Generate content using the model
response = model.generate_content(prompt)
time_response=model.generate_content(time_based_prompt)
cuisine_response=model.generate_content(cuisine_based_prompt)
# Print the generated content
print(response.text)
print("--------------------------------------------------------------------------------------")
print(time_response.text)
print("--------------------------------------------------------------------------------------")
print(cuisine_response.text)

import torch
import numpy as np
from PIL import Image
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Load YOLOv5 model
def load_yolov5_model(model_path):
    model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, force_reload=True)
    model.eval()
    return model

# Detect ingredients from an image using YOLOv5
def detect_ingredients(model, img_path):
    img = Image.open(img_path)
    results = model(img)
    ingredients = results.pandas().xyxy[0]['name'].tolist()
    return ingredients

# Encode ingredients into a format suitable for TensorFlow model
def encode_ingredients(ingredients, all_possible_ingredients):
    return np.array([[1 if ingredient in ingredients else 0 for ingredient in all_possible_ingredients]])

# Build TensorFlow model
def build_model(input_dim):
    model = Sequential([
        Dense(128, activation='relu', input_shape=(input_dim,)),
        Dropout(0.5),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(1, activation='linear')  # Output layer for regression; predicts time
    ])
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Predict cooking time
def predict_cooking_time(model, ingredient_features):
    return model.predict(ingredient_features)[0][0]

# Full prediction workflow
def full_prediction_workflow(img_path, yolo_model, tf_model, all_possible_ingredients):
    detected_ingredients = detect_ingredients(yolo_model, img_path)
    ingredient_features = encode_ingredients(detected_ingredients, all_possible_ingredients)
    predicted_time = predict_cooking_time(tf_model, ingredient_features)
    return predicted_time

# Paths and parameters
yolo_model_path = '/content/yolov5/runs/train/exp3/weights/last.pt'
yolo_model = load_yolov5_model(yolo_model_path)
all_possible_ingredients = ['apple', 'banana', 'beef', 'blueberries', 'bread', 'butter', 'carrot', 'cheese', 'chicken', 'chicken_breast', 'chocolate', 'corn', 'eggs', 'flour', 'goat_cheese', 'green_beans', 'ground_beef', 'ham', 'heavy_cream', 'lime', 'milk', 'mushrooms', 'onion', 'potato', 'shrimp', 'spinach', 'strawberries', 'sugar', 'sweet_potato', 'tomato']

tf_model = build_model(len(all_possible_ingredients))  # Make sure to train this model first!

# Example usage
img_path = '/content/yolov5/runs/train/exp3/weights/sample.png'
predicted_time = full_prediction_workflow(img_path, yolo_model, tf_model, all_possible_ingredients)
print(f"Cooking time for ingredients in {img_path} is approximately {predicted_time:.2f} minutes")
